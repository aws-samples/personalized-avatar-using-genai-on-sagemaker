# Personalized avatar using Amazon EC2 Accelerated Computing instances including AWS Inferentia. 


## Overview
This repo demonstrates how to calibrate and deploy a Stable Diffusion (SD) model to generate personalized avatars with a simple text prompt using [Amazon EC2](https://aws.amazon.com/ec2/). **SD** is a text-to-image model, generated by a type of artificial intelligence (AI) that leverages the latest advances in machine learning. 

Here, the models are calibrated using the DreamBooth and Low-Rank Adaptation (LoRA) fine tuning, which uses 10-15 images of the user to capture the precise details of the subject. The models are then served using [Nvidia Triton Inference Server](https://developer.nvidia.com/triton-inference-server) to save inference cost by share and reuse hosting infrastructure. 

The repo is structured into two sections: 1) fine tune and host SD model using nvidia GPU EC2 instances, and 2) prepare and host SD model using Amazon Inferentia EC2 instances.


### Prerequisites

Make sure you have an AWS account and has permissions to create AWS Resources (SageMaker notebook instance, EC2, and etc).

Then clone the repository using the follow command (code is available in `ec2-inf2` branch):

```bash
git clone -b ec2-inf2 https://github.com/aws-samples/personalized-avatar-using-genai-on-sagemaker.git
```


## [nvidia-g5-ec2](1-nvidia-g5-ec2/)

This section focus on fine tune and host SD model using nvidia GPU EC2 instances. There are two Jupyter notebook files: The first notebook, `1_personalized_avatar_solution.ipynb`, shows how to fine-tune Stable Diffusion using DreamBooth and LoRA to create a model tailored to a specific person. The second notebook, `2_sd_triton_mme.ipynb`, packages the fine-tuned model and sets up an inference endpoint using NVIDIA Triton Inference Server. By running these two notebooks, you can create a Inf2 instance (`inf2.8xlarge or larger`) and personalized avatar model for a specific person and deploy it for inference.

<div class="alert alert-warning">
<b>Warning</b>: The notebooks are tested on on `conda_python_p310` kernel on SageMaker notebook instance. An GPU instance such as `ml.g5.2xlarge` is recommended.
</div>


## [aws-inf2-ec2](2-aws-inf2-ec2/)

This section focus on prepare and host SD model using Amazon Inferentia EC2 instances. To run the latest PyTorch Neuron Deep Learning AMI. Please reference the [AWS Neuron Documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/neuron-setup/pytorch/neuronx/ubuntu/torch-neuronx-ubuntu20-pytorch-dlami.html#setup-torch-neuronx-ubuntu20-dlami-pytorch) to get started.


Run first notebook ,`1_hf_lora_sd2_512_inference.ipynb`, to compile and run a LoRA fine-tuned Stable Diffusion 2.1 (512x512) model for accelerated inference on Neuron. Then the sceond notebook, `2_sd-triton-mme.ipynb`, host the fine tuned model wtih Triton Inference Server on AWS Inferentia 2.

<div class="alert alert-warning">
<b>Warning</b>: The notebooks should run on an Inf2 instance (`inf2.8xlarge` or larger).
</div>


## Cleanup

Follow the instructions in the cleanup section of the notebook to delete the resources provisioned as part of this post to avoid unnecessary charges.


## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the MIT-0 License. See the LICENSE file.
