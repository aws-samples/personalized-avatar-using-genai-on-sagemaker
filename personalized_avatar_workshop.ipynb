{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89873d48-d5a8-4f6b-a689-8b1e083265c3",
   "metadata": {},
   "source": [
    "# Building Personalized Avatar Using Generative AI using Amazon SageMaker\n",
    "\n",
    "Generative AI has become a popular tool for enhancing and accelerating the creative process across various industries, including entertainment, advertising, and art. It enables more personalized experiences for audiences and improves the overall quality of the final products. \n",
    "\n",
    "In workshop, you will fine-tune a Stable Diffusion (SD) model to build a personalized avatar generator on Amazon SageMaker and save inference cost with Multi Model Endpoints (MME) at the same time. \n",
    "\n",
    "The entire example takes about 1 hour to complete. At the end, you will have a simple [Gradio](https://www.gradio.app/) application to experiment with different prompt and generate avatar images of yourself.\n",
    "\n",
    "\n",
    "Recommend to use `Data Science 3.0` kernel in SageMaker Studio with a `ml.m5.large` instance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790bc554-499b-4b62-b1cd-5e74627d23a1",
   "metadata": {},
   "source": [
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [Prepare Image Data](#2.-Prepare-Image-Data)\n",
    "3. [Run LoRA Finetuning](#3.-Run-LoRA-Finetuning)\n",
    "4. [Host Multi-Model Endpoints](#4.-Host-Multi-Model-Endpoints)\n",
    "5. [Invoke Model](#5.-Invoke-the-utility-model)\n",
    "6. [Run The Gradio App](#6.-Run-The-Gradio-App)\n",
    "7. [Clean Up](#7.-Clean-Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea7843-825b-4e8d-bced-e573b95aca37",
   "metadata": {},
   "source": [
    "### 1. Set Up\n",
    "\n",
    "Installs the dependencies required to package the model and test the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dbb37-003b-458c-afcf-37be47c08b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq diffusers\n",
    "!pip install -Uq peft\n",
    "!pip install -Uq conda-pack\n",
    "!pip install -Uq gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad82cc-17e7-4628-9dac-5c2727ea873b",
   "metadata": {},
   "source": [
    "#### Permissions and environment variables\n",
    "\n",
    "***\n",
    "To use Amazon SageMaker, you need to set up and authenticate the use of AWS services. Here, you use the execution role associated with the current notebook as the AWS account role with SageMaker access.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d3fe99-b726-4a27-b15c-beff90ff8f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.experiments.run import Run\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.model import Model\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "import time\n",
    "from io import BytesIO\n",
    "import os\n",
    "import tarfile\n",
    "import base64\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfee20-56b5-422e-bdad-c149a926d850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_prefix = (\n",
    "    \"stable-diffusion-dreambooth-workshop\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bc230-8562-42c5-ac14-65969b855f41",
   "metadata": {},
   "source": [
    "### 2. Prepare Image Data\n",
    "\n",
    "**<span style=\"color:red\">IMPORTANT</span>: upload minimum 10 images of yourself to the `data` folder.** The images needs to capture the essence of how you look clearly from multiple perspectives. Include a front-facing photo, a profile shot from each side, and photos from angles in between. You should also include photos with different facial expressions like smiling, frowning, and a neutral expression. Having a mix of expressions will allow the model to better reproduce your unique facial features. \n",
    "\n",
    "---\n",
    "\n",
    "Then you will upload these images to a S3 location for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba792a-8239-4206-8408-8efc09f1b722",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_path = \"data\"\n",
    "s3_path = f\"s3://{bucket}/{s3_prefix}/images\"\n",
    "\n",
    "print(f\"your images are uploaded here: {s3_path}\\n\\n\")\n",
    "print(\"----------------------\\n\")\n",
    "!aws s3 cp {local_path} {s3_path} --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb36f4-652c-4871-befa-4f898d1bc454",
   "metadata": {},
   "source": [
    "### 3. Run LoRA Finetuning\n",
    "\n",
    "You are going to use SageMaker's Hugging Face Training Estimator to fine-tune a personalized avatar model. This uses a managed container with HuggingFace Transformers libraries, enabling easy training of transformer based models. (Training takes about 26 mins)\n",
    "\n",
    "if you take a closer look at the training scripts in `src` folder. Here is an overview of what they are. \n",
    "\n",
    "```\n",
    "|-- src                      Training code directory for the fine tuning job\n",
    "|   |--launch.py             Entry script for the training job\n",
    "|   |--requirements.txt      Python modules to extend the container\n",
    "|   |--trainer.py            LoRA fine tuning script\n",
    "|   |--train_dreambooth.py   Dreambooth script\n",
    "|   |--utils.py              Utility functions\n",
    "    └── sd_lora              A Triton Python backend model template directory for LoRA fine-tuned Stable Diffusion models\n",
    "        |-- 1\n",
    "        |   └── model.py\n",
    "        └── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a7600-b3b5-42af-85f7-0fb656206d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"input_data\": \"/opt/ml/input/data/training\",\n",
    "    \"resolution\": 512,\n",
    "    \"num_steps\": 1000,\n",
    "    \"concept_prompt\": \"photo of <<TOK>>\", # Fine tuned instance\n",
    "    \"class_prompt\": \"a photo of person\", # Fine tuned class\n",
    "    \"lr\": 1e-4,\n",
    "    \"grad_accum\": 1,\n",
    "    \"train_text_encoder\": True,      # to change, you need to comment this out instead of change to False\n",
    "    \"prior_preservation\": True,      # to change, you need to comment this out instead of change to False\n",
    "    \"prior_loss_weight\": 1.0,\n",
    "    \"num_class_images\": 50,\n",
    "    \"lora_r\": 128,\n",
    "    \"lora_alpha\": 1,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_text_encoder_r\": 64,\n",
    "    \"lora_text_encoder_alpha\": 1,\n",
    "    \"lora_text_encoder_dropout\": 0.05,\n",
    "    \"face_preprocessing\":True,\n",
    "}\n",
    "\n",
    "# Define the configuration for the training job\n",
    "estimator = HuggingFace(\n",
    "    entry_point=\"launch.py\",\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    transformers_version=\"4.26\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    py_version=\"py39\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    keep_alive_period_in_seconds=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807ff02-fe81-4c19-9401-4248481df03f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "experiment_name = \"picture-book-statble-diffusion\"\n",
    "run_name = name_from_base(\"lora-with-dreambooth\")\n",
    "with Run(experiment_name=experiment_name, sagemaker_session=sess, run_name=run_name) as run:\n",
    "    \n",
    "    # Start the training job\n",
    "    data = {'training': s3_path}\n",
    "\n",
    "    estimator.fit(data)#, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02050b-4967-432c-a928-862cf6fd1f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_model_path = estimator.model_data\n",
    "\n",
    "%store initial_model_path\n",
    "\n",
    "print(f\"model.tar.gz is located here: {initial_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc56ccb2-dda7-4698-a0e8-627c1ae2d08e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Host Multi-Model Endpoints\n",
    "\n",
    "You are going to use SageMaker MME to host your personalized avatar models. MME allows you to define a central S3 location to host your models and SageMaker take care of loading and caching your model dynamically base on your traffic patterns. This hosting approach optimizes resource utilization, save costs, and minimize operational burden of managing thousands of endpoints.\n",
    "\n",
    "\n",
    "As of today, MME with GPU support NVIDIA Triton as the model server. For more detail on MME triton, please visit refer to this [blog](https://aws.amazon.com/blogs/machine-learning/run-multiple-deep-learning-models-on-gpu-with-amazon-sagemaker-multi-model-endpoints/)\n",
    "\n",
    "At a high level, model serving using Triton require certain model package format. When using Python backend, a triton config file and a Python script are required. The python script has to be named `modle.py`, and final `model.tar.gz` file should have file structure similar to below:\n",
    "\n",
    "```\n",
    "|--sd_lora\n",
    "   |--config.pbtxt        # triton server configuration\n",
    "   |--1\\\n",
    "      |--model.py         # inference handler script\n",
    "    ...\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09285484-88c1-41fd-84e6-aac77d3d4798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the central MME S3 bucket location\n",
    "mme_prefix = f\"{s3_prefix}/inference/models\"\n",
    "\n",
    "model_data_url = f\"s3://{bucket}/{mme_prefix}/\"\n",
    "\n",
    "print(f\"MME S3 bucket: {model_data_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8f30f-fd13-48ad-804c-3f29eacc1c3c",
   "metadata": {},
   "source": [
    "SD can be difficult to dynamically load due to its large model size. A full SD model is  about 6GB on disk, and presents a challenge during intial load. Cold start can take well over 60s to download and unpackage the model files. To optimize this, you have to de-couple the base SD model from it's fine-tuned LoRA weights. The diagram bellow illustrate a new design to centrally share the base SD model and it's conda environment from a central location.\n",
    "\n",
    "<img src=\"statics/mme_diagram.png\">\n",
    "\n",
    "When a model needs to be loaded from S3 the first time, you `model.tar.gz` will only contain the LoRA weights (68 MB) comparing to the entire SD model (~6GB).\n",
    "\n",
    "**Note: this only shares the storage of models. For GPU memory, this still need to load a full SD model for every model instance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27d377-983f-4654-8fe2-46d55fa6fbdc",
   "metadata": {},
   "source": [
    "#### Prepare the utility model\n",
    "\n",
    "To accomplish this without building a custom container, you will have build an utility model to pre-load shared resurces onto the instance. A template of this utility model is located at `models/model_setup` folder.\n",
    "\n",
    "The utility model does 2 things when invoked:\n",
    "\n",
    "1. upload a conda pack from S3. This contains all the requirement modules to run the model\n",
    "2. upload the base SD model from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2ab78-7fff-4695-92f4-82f47d6d0309",
   "metadata": {},
   "source": [
    "#### 1. Prepare the conda pack\n",
    "\n",
    "When using the Triton Python backend (which our Stable Diffusion model will run on), you can include your own environment and dependencies. The recommended way to do this is to use [conda pack](https://conda.github.io/conda-pack/) to generate a conda environment archive in `tar.gz` format, and point to it in the `config.pbtxt` file of the models that should use it, adding the snippet: \n",
    "\n",
    "```\n",
    "parameters: {\n",
    "  key: \"EXECUTION_ENV_PATH\",\n",
    "  value: {string_value: \"path_to_your_env.tar.gz\"}\n",
    "}\n",
    "\n",
    "```\n",
    "You can use a different environment per model, or the same for all models (read more on this [here](https://github.com/triton-inference-server/python_backend#creating-custom-execution-environments)). Since the all of the models that we'll be deploying have the same set of environment requirements, you will create a single conda environment and will use a Python backend to copy that environment into a location where it can be accessed by all models.\n",
    "\n",
    "> ⚠ **Warning**: The approach for a creating a shared conda environment highlighted here is limited to a single instance deployment only. In the event of auto-scaling, there is no guarantee that the new instance will have the conda environment configured. Since the conda environment for hosting Stable Diffusion models is quite large  the recommended approach for production deployments is to create shared environment by extending the Triton Inference Image.  \n",
    "\n",
    "Let's start by creating the conda environment with the necessary dependencies; running these cells will output a `sd_env.tar.gz` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd0a34-75f5-48e2-91fa-c0cf43687065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile environment.yml\n",
    "name: mme_env\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pip\n",
    "  - pip:\n",
    "      - numpy\n",
    "      - torch --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "      - accelerate\n",
    "      - transformers\n",
    "      - diffusers\n",
    "      - xformers\n",
    "      - peft\n",
    "      - conda-pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceea382-2b29-487b-9b1a-abd26cd72b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc44552-61e6-4093-9efc-1b8e6235c5ad",
   "metadata": {},
   "source": [
    "Now you can create the environment using the above environment yaml spec\n",
    "\n",
    "It could take up to 5 min to create the conda environment. The packaged conda environment will be stored in `models/model_setup/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8195fc-879d-424b-bb8f-57f428a21bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda pack -n mme_env -o models/model_setup/sd_env.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40840e4-175e-4b8f-b4bd-f9c2694989e7",
   "metadata": {},
   "source": [
    "#### Prepare the stable diffusion base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7c814-be0e-428d-ae6d-c43061f2f658",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import diffusers\n",
    "import torch \n",
    "from peft import PeftModel\n",
    "import os\n",
    "\n",
    "device=\"cuda\"\n",
    "\n",
    "\n",
    "pipe = diffusers.StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\",\n",
    "                                                             cache_dir='hf_cache',\n",
    "                                                             torch_dtype=torch.float16,\n",
    "                                                             revision=\"fp16\")\n",
    "\n",
    "# save the base model, you will need to use this for inference.\n",
    "sd_dir = 'stable_diff'\n",
    "pipe.save_pretrained(sd_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e87ffd-5c9f-4dac-ba6d-fbb063cf39db",
   "metadata": {},
   "source": [
    "Store this into `models/model_setup/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d50020-68fe-46f2-931c-f21334f1768b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sd_tar = f\"models/model_setup/{sd_dir}.tar.gz\"\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "        \n",
    "    print(f\"SD base model created here: {output_filename}\")\n",
    "\n",
    "make_tarfile(sd_tar, sd_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a815a64e-cfaa-42fa-bcc0-47dbc0ce5c8f",
   "metadata": {},
   "source": [
    "Upload the utility model to S3 bucket (this may take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9435299-081e-4bc3-96a7-b98daad22805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c389b-b0b0-4948-8851-d67cb8655a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_repo = \"models\"\n",
    "\n",
    "model_name = \"model_setup\"\n",
    "tar_name = f\"{model_name}.tar.gz\"\n",
    "!tar -C $model_repo -czvf $tar_name $model_name\n",
    "sess.upload_data(path=tar_name, bucket=bucket, key_prefix=mme_prefix)\n",
    "!rm $tar_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68b52a-fd0b-4725-8636-e1fb8863f48b",
   "metadata": {},
   "source": [
    "#### Deploy endpoint\n",
    "Now, you get the correct URI for the SageMaker Triton container image. Check out all the available Deep Learning Container images that AWS maintains [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1021b322-4e48-4bf9-81b1-0baf8d1f695e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# account mapping for SageMaker Triton Image\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.12-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34de6779-44bc-49d3-b72c-555d52aa6750",
   "metadata": {},
   "source": [
    "you are now ready to configure and deploy the multi-model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec38e4d-6222-4db7-8ab9-103ed8612481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "\n",
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": model_data_url,     # S3 location of the models\n",
    "    \"Mode\": \"MultiModel\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6175059c-280a-47cd-8e97-2063b75f1b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_model_name = name_from_base(f\"{mme_prefix.split('/')[0]}-models\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e53d97-7bbc-424a-913a-43b8d3e90560",
   "metadata": {},
   "source": [
    "Create a SageMaker endpoint configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d709bc4-f36e-47b4-bdf0-883b11d9af58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = name_from_base(f\"{mme_prefix.split('/')[0]}-epc\")\n",
    "\n",
    "instance_type = 'ml.g5.xlarge'\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e5544-30a6-47fc-ad88-557eb9c11a6b",
   "metadata": {},
   "source": [
    "Create the endpoint, and wait for it to transition to InService state. (This takes about 5 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be35d99-9e61-4bf0-a551-4fcb70bd39e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = name_from_base(f\"{mme_prefix.split('/')[0]}-ep\")\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db470b6-5ed9-441d-8fc9-be7c85bdf27c",
   "metadata": {},
   "source": [
    "### 5. Invoke the utility model\n",
    "Prior to invoking any of the personalized avatar models, you first invoke the utility model to load the conda environment and stable diffusion base model Refer to the [model.py](./models/model_setup/1/model.py) file in the `models/model_setup/1` directory for more details on the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee13cf0-6f56-4af1-824a-93b602c31a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# invoke the setup_conda model to create the shared conda environment\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "inputs = dict(input_args = \"hello\")\n",
    "\n",
    "payload = {\n",
    "    \"inputs\":\n",
    "        [{\"name\": name, \"shape\": [1,1], \"datatype\": \"BYTES\", \"data\": [data]} for name, data in inputs.items()]\n",
    "}\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"model_setup.tar.gz\",\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))[\"outputs\"]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a65265-c518-4372-b977-e57667fec18b",
   "metadata": {},
   "source": [
    "#### Invoke the LoRA fine tuned model\n",
    "\n",
    "To demonstrate the capability of MME, you will make multiple copy of our fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83a19f-8da7-42a5-ad83-c94415b29b86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_loaded = []\n",
    "model_count = 0\n",
    "\n",
    "# create a dummy payload ====================\n",
    "prompt = \"<<TOK>>\"\n",
    "negative_prompt = \"\"\n",
    "\n",
    "gen_args = json.dumps(dict(num_inference_steps=50, guidance_scale=7, seed=0))\n",
    "\n",
    "inputs = dict(prompt = prompt,\n",
    "              negative_prompt = negative_prompt,\n",
    "              gen_args = gen_args)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\":\n",
    "        [{\"name\": name, \"shape\": [1,1], \"datatype\": \"BYTES\", \"data\": [data]} for name, data in inputs.items()]\n",
    "}\n",
    "\n",
    "# make replica and load the models to MME S3 location\n",
    "\n",
    "for x in range(3):\n",
    "    # make a copy of the model\n",
    "    model_name = f\"avatar-model-v{x}.tar.gz\"\n",
    "    !aws s3 cp {initial_model_path} {model_data_url}{model_name}\n",
    "    \n",
    "    # make a inference request to load model into memory\n",
    "    response = sm_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=model_name, \n",
    "        )\n",
    "    \n",
    "    models_loaded.append(model_name)\n",
    "        \n",
    "    model_count+=1\n",
    "\n",
    "print(f\"\\n\\nLoaded {model_count} personalized avatar models \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95419e-bc03-49f5-8e21-dbeab9b4b50f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper functions to encode and decode images\n",
    "def decode_image(img):\n",
    "    buff = BytesIO(base64.b64decode(img.encode(\"utf8\")))\n",
    "    image = Image.open(buff)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c58a56-d515-480a-b4dc-fd7831b24d0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "prompt = \"TOK style illustration of a boy in a yard, cute, smiling, trees in the background, high Res, 2 dimension\"\n",
    "\n",
    "# prompt = \"\"\"<<TOK>> epic portrait, zoomed out, blurred background cityscape, bokeh, perfect symmetry, by artgem, artstation ,concept art,cinematic lighting, highly detailed, \n",
    "# octane, concept art, sharp focus, rockstar games,\n",
    "# post processing, picture of the day, ambient lighting, epic composition\"\"\"\n",
    "negative_prompt = \"\"\"\n",
    "beard, goatee, ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, \n",
    "watermark, grainy, signature, cut off, draft, amateur, multiple, gross, weird, uneven, furnishing, decorating, decoration, furniture, text, poor, low, basic, worst, juvenile, \n",
    "unprofessional, failure, crayon, oil, label, thousand hands\n",
    "\"\"\"\n",
    "\n",
    "seed = random.randint(1, 1000000000)\n",
    "gen_args = json.dumps(dict(num_inference_steps=50, guidance_scale=7, seed=seed))\n",
    "\n",
    "inputs = dict(prompt = prompt,\n",
    "              negative_prompt = negative_prompt,\n",
    "              gen_args = gen_args)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\":\n",
    "        [{\"name\": name, \"shape\": [1,1], \"datatype\": \"BYTES\", \"data\": [data]} for name, data in inputs.items()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3c338-ddfe-4817-9d3d-452d8df6ede9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=models_loaded[0],\n",
    ")\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))[\"outputs\"]\n",
    "original_image = decode_image(output[0][\"data\"][0])\n",
    "original_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726cac2-e5e5-4eec-9837-c682ba4c2abc",
   "metadata": {},
   "source": [
    "## 6. Run The Gradio App\n",
    "\n",
    "Gradio is an open-source Python library that allows developers to easily create and share custom web-based interfaces for their machine learning models, without requiring any web development skills. \n",
    "\n",
    "After you have installed Gradio, run the code below. The interative UI will render directly in the output cell. You can interact with your models and generate avatars. Have fun :)\n",
    "\n",
    "---\n",
    "\n",
    "**Example prompt:** front portrait, with glasses, zoomed out, young and handsome, perfectly centered, anime, cute-fine-face, illustration, realistic shaded perfect face, fine details, image premiere, 4k resolution, a masterpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b4936-84be-4f2b-adc5-8d6acc0ddd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Personalized Avatar Generator\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "\n",
    "            models = gr.Dropdown(choices=models_loaded, type=\"value\",\n",
    "                                 info=\"Choose a model\", show_label=False)\n",
    "\n",
    "            prompt = gr.Textbox(show_label=False,\n",
    "                                info=\"Prompt:\",\n",
    "                                placeholder=\"Enter a prompt for your avatar\")\n",
    "            nprompt = gr.Textbox(show_label=False,\n",
    "                                 info=\"Negative prompt:\",\n",
    "                                 placeholder=\"\"\"beard, goatee, ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, \n",
    "watermark, grainy, signature, cut off, draft, amateur, multiple, gross, weird, uneven, furnishing, decorating, decoration, furniture, text, poor, low, basic, worst, juvenile, \n",
    "unprofessional, failure, crayon, oil, label, thousand hands\n",
    "\"\"\")\n",
    "\n",
    "            create = gr.Button(value=\"Create\")\n",
    "        with gr.Column(scale=1):\n",
    "            output_img = gr.Image(label=\"Output Image\", type=\"pil\", height=400)\n",
    "\n",
    "\n",
    "    def generate_avatar(model_name, p, np, inf_steps=50, scale=10):\n",
    "        \n",
    "        s = random.randint(1, 1000000000)\n",
    "        \n",
    "        gen_args = json.dumps(dict(num_inference_steps=inf_steps, guidance_scale=scale, seed=s))\n",
    "\n",
    "        inputs = dict(prompt = f\"<<TOK>>, {p}\",\n",
    "                      negative_prompt = np,\n",
    "                      gen_args = gen_args)\n",
    "\n",
    "        payload = {\n",
    "            \"inputs\":\n",
    "                [{\"name\": name, \"shape\": [1,1], \"datatype\": \"BYTES\", \"data\": [data]} for name, data in inputs.items()]\n",
    "        }\n",
    "        \n",
    "        response = sm_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=model_name,\n",
    "        )\n",
    "        output = json.loads(response[\"Body\"].read().decode(\"utf8\"))[\"outputs\"]\n",
    "        output_image = decode_image(output[0][\"data\"][0])\n",
    "        \n",
    "        return output_image\n",
    "\n",
    "    create.click(generate_avatar, [models, prompt, nprompt], output_img)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a15cc-e0c0-4e34-8cee-bd01aec434d0",
   "metadata": {},
   "source": [
    "## 7. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92af07-2bbd-41f5-9ddb-246f78f471e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=sm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6788cd4-d81e-438c-880c-ebf3ad4bcb74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
