{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6482139c",
   "metadata": {},
   "source": [
    "# Deploy Stable Diffusion using Triton\n",
    "\n",
    "In this notebook we will host LoRA finetuned Stable Diffusion models on Triton Inference Server provided by NVIDIA\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Warning</b>: This notebook is tested on `torch-neuronx` kernel an Inf2 instance (`inf2.8xlarge or larger`)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bb967",
   "metadata": {},
   "source": [
    "### Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e6d37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]\n",
    "!pip install -U sagemaker pywidgets numpy PIL\n",
    "!pip install -Uq conda-pack==0.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import *\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "# variables\n",
    "s3_client = boto3.client(\"s3\")\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74d03f-3494-402f-a942-451738b62bab",
   "metadata": {},
   "source": [
    "### Setup Triton on Inf2\n",
    "\n",
    "On the Inf2 instances, run the follow command to install the runtime tools on the instance.\n",
    "\n",
    "```bash\n",
    " $chmod 777 setup-pre-container.sh\n",
    " $sudo setup-pre-container.sh -inf2\n",
    "```\n",
    "This installs the following runtime tools on the instance (Your DLAMI may already have these pre-installed):\n",
    "\n",
    "```\n",
    "aws-neuronx-dkms=2.* \\\n",
    "aws-neuronx-tools=2.* \\\n",
    "aws-neuronx-collectives=2.* -y \\\n",
    "aws-neuronx-runtime-lib=2.* -y\n",
    "```\n",
    "\n",
    "Then build the custom container to install the requirements and, most importantly, run the `setup.sh` scrip to properly install neuron compiler and neuron framework packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db71af-77eb-4a5c-a66f-d6624a230755",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat docker/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e1748-09d9-4754-af08-800cbd9e1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image_name = f\"tritonserver-pt-inf2\"\n",
    "base_image = \"nvcr.io/nvidia/tritonserver:23.03-py3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96526e-c84c-41f4-8ab1-f5748e668311",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture build_output\n",
    "!cd docker && docker build  -t {new_image_name} . --build-arg BASE_IMAGE={base_image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201ab16-4a43-47c0-ad74-73c061beb5fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(build_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e5b1df-f320-4e0c-b39a-d4a088e9939c",
   "metadata": {},
   "source": [
    "list the docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66914aa-bae7-4e45-b859-1bc9c1d16d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76710776-e04c-48a1-8e7b-ba5cec67c72d",
   "metadata": {},
   "source": [
    "## What is Triton Inference Server\n",
    "\n",
    "**Triton Inference Server** is an open source inference serving toolkit from NVIDIA that supports high-performance inferencing for deep learning models. It provides a framework-agnostic platform to deploy trained AI models from any framework, including TensorFlow, PyTorch, and ONNX. Triton allows multiple models to be served from the same server, optimizing hardware utilization.\n",
    "\n",
    "**The Triton backend for Python.** The goal of Python backend is to let you serve models written in Python by Triton Inference Server without having to write any C++ code. Read [here](https://github.com/triton-inference-server/python_backend) for more information\n",
    "\n",
    "In this example, a fine tuned stable diffusion models are already prepared for you. Take a look at the `model_repository` folder structure.\n",
    "\n",
    "```\n",
    "model_repository\n",
    "└── james                                       # model folder\n",
    "    ├── 1                                       # model version\n",
    "    │   └── model.py                            # inference handler  must save in this python file\n",
    "    │   └── sd2_compile_dir_512                 # compiled sd model (generated from other notebook)\n",
    "    └── config.pbtxt                            # model configuration\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd56b9b-d089-4bc2-8b29-5fadc512afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0912969-1f84-47f4-9b42-d24eaa7cea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"model_repository\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8bfb54-efa2-44b4-9457-c80a7acb04f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --device /dev/neuron0 -d --shm-size=4G --rm -p8000:8000 -p8001:8001 -p8002:8002 -v$(pwd)/$repo_name:/model_repository tritonserver-pt-inf2:latest tritonserver --model-repository=/model_repository --exit-on-error=false\n",
    "time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c1e5e-b03c-4032-89a5-e555a4ad0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_ID=!docker container ls -q\n",
    "FIRST_CONTAINER_ID = CONTAINER_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04604e6-881f-43a7-a958-2a8fe2d5518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $FIRST_CONTAINER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463cc1bf-c6cb-4043-8f69-5581133322a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker logs $FIRST_CONTAINER_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85a987-c9b7-4179-af32-a35517b3d0dd",
   "metadata": {},
   "source": [
    "#### Now we will invoke the script locally\n",
    "\n",
    "We will use Triton's HTTP client and its utility functions to send a request to `localhost:8000`, where the server is listening. We are sending text as binary data for input and receiving an array that we decode with numpy as output. Check out the code in `model_repository/pipeline/1/model.py` to understand how the input data is decoded and the output data returned, and check out more Triton Python backend [docs](https://github.com/triton-inference-server/python_backend) and [examples](https://github.com/triton-inference-server/python_backend/tree/main/examples) to understand how to handle other data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084fab8c-379b-45d1-9f37-63b863a43751",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = httpclient.InferenceServerClient(url=\"localhost:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290f20d-e598-46f8-9d5a-87035d03056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "prompt = \"\"\"\n",
    "photo of <<TOK>> pencil sketch, young and handsome, face front, centered\n",
    "\"\"\"\n",
    "\n",
    "negative_prompt = \"\"\"\n",
    "ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, \n",
    "watermark, grainy, signature, cut off, draft, amateur, multiple, gross, weird, uneven, furnishing, decorating, decoration, furniture, text, poor, low, basic, worst, juvenile, \n",
    "unprofessional, failure, crayon, oil, label, thousand hands\n",
    "\"\"\"\n",
    "\n",
    "seed = random.randint(1, 1000000000)\n",
    "gen_args = json.dumps(dict(num_inference_steps=50, guidance_scale=7, seed=seed))\n",
    "\n",
    "input_dict = dict(prompt = prompt,\n",
    "              negative_prompt = negative_prompt,\n",
    "              gen_args = gen_args)\n",
    "inputs = []\n",
    "for name, data in input_dict.items():\n",
    "    \n",
    "    obj = np.array([data], dtype=\"object\").reshape((-1, 1))\n",
    "\n",
    "    i = httpclient.InferInput(name, obj.shape, np_to_triton_dtype(obj.dtype))\n",
    "    i.set_data_from_numpy(obj)\n",
    "    inputs.append(i)\n",
    "\n",
    "output_img = httpclient.InferRequestedOutput(\"generated_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b071e1a-a166-45f6-b841-78b5c9ad583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = \"james\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0652d-26b8-4024-88eb-0ee8e5fd28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "query_response = client.infer(model_name=target_model, inputs=inputs, outputs=[output_img])\n",
    "\n",
    "print(f\"took {time.time()-start} seconds\")\n",
    "\n",
    "image = query_response.as_numpy(\"generated_image\")\n",
    "\n",
    "test = np.squeeze(image).tolist()\n",
    "Image.open(BytesIO(base64.b64decode(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c177116-edce-4293-bbba-2b3d9d69c373",
   "metadata": {},
   "source": [
    "To check if neuron is being used: run `neuron-top` command on the instance.\n",
    "\n",
    "(Note: Device Memory  should be non-zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab980b-835f-4c90-81ba-0610970492ab",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f73a5-1f87-44e0-88e0-1e05fc4f9b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker kill $FIRST_CONTAINER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d2d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-neuronx)",
   "language": "python",
   "name": "aws_neuron_venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
