{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Personalized Avatar  Using Generative AI\n",
    "\n",
    "Generative AI has become a popular tool for enhancing and accelerating the creative process across various industries, including entertainment, advertising, and art. It enables more personalized experiences for audiences and improves the overall quality of the final products. \n",
    "\n",
    "In this notebook, we will demonstrate how you can use generative AI models like Stable Diffusion (SD) to build a personalized avatar generator using Amazon EC2.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Warning</b>: You should run this notebook on a SageMaker Notebook Instance. An GPU instance such as `ml.g5.2xlarge` is recommended. This notebook is tested on `conda_python_p310` kernel. \n",
    "</div>\n",
    "---\n",
    "\n",
    "The entire example takes about 1 hour to complete. Here is the cost breakdown:\n",
    "\n",
    "- `ml.g5.2xlarge` instance is $1.52 per hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "Installs the dependencies required to package the model and test the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq diffusers==0.21.4\n",
    "!pip install -Uq accelerate==0.22.0\n",
    "!pip install -Uq peft==0.4.0\n",
    "!pip install -Uq conda-pack==0.7.1\n",
    "!pip install -Uq gradio==3.41.2\n",
    "!pip install -Uq autocrop==1.3.0\n",
    "!pip install -Uq datasets\n",
    "!pip install -Uq bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the diffusers version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "\n",
    "# check difusers version, make sure it's 0.21.4\n",
    "diffusers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the images\n",
    "\n",
    "The sample images are provided in the `data` folder. You can also replace the images with your own. You should also include photos with different facial expressions like smiling, frowning, and a neutral expression. Having a mix of expressions will allow the model to better reproduce your unique facial features. The input images dictate the quality of avatar you can generate. \n",
    "\n",
    "The accepted formats are `.jpg` or `.png`. \n",
    "\n",
    "<img src=\"statics/input_examples.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help the model focus on the facial features, we implement a preprocessing step using computer vision techniques to face detect and center crop the faces from images. This alleviate the burden for user to curiate the perfect images for the model. \n",
    "\n",
    "The preprocessing code is in `utils.py` where we first use a face detection model to isolate the largest face in each image. Then we crop and pad the image to the required size of 512 x 512 pixels for our model. \n",
    "\n",
    "\n",
    "<img src=\"statics/prepare_images.jpg\" alt=\"image\" width=\"300\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "import utils\n",
    "import shutil\n",
    "\n",
    "imag_dir=Path(\"data\")\n",
    "dest_dir = Path(\"cropped\")\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for n,img_path in enumerate(chain(imag_dir.glob(\"*.[jJ][pP]*[Gg]\"),imag_dir.glob(\"*.[Pp][Nn][Gg]\"))):\n",
    "    try:\n",
    "        cropped = utils.detect_face_and_resize(img_path.as_posix())\n",
    "        cropped.save(dest_dir / f\"image_{n}.png\")\n",
    "    except ValueError:\n",
    "        print(f\"Could not detect face in {img_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "print(\"Here are the preprocessed images ==========\")\n",
    "[x.as_posix() for x in dest_dir.iterdir() if x.is_file()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Stable Diffusion Model\n",
    "\n",
    "[DreamBooth](https://arxiv.org/abs/2208.12242) is a method to personalize text2image models like stable diffusion given just a few images of a subject. \n",
    "\n",
    "[Low-Rank Adaption (LoRA)](https://arxiv.org/abs/2106.09685) is a parameter effecient fine tuning technique that adapt pretrained models by adding pairs of rank-decomposition matrices to existing weights and only training those newly added weights. This has a couple of advantages:\n",
    "\n",
    "- Previous pretrained weights are kept frozen so that the model is not prone to catastrophic forgetting\n",
    "- Rank-decomposition matrices have significantly fewer parameters than the original model, which means that trained LoRA weights are easily portable.\n",
    "- LoRA attention layers allow to control to which extent the model is adapted towards new training images via a scale parameter.\n",
    "\n",
    "The `train_dreambooth_lora.py` script shows how to implement the training procedure with dreambooth and LoRA for stable diffusion. More implementations available ind [diffusers examples](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "lora_dir = output_dir / \"lora\"\n",
    "lora_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are example parameters you can configure for fine tuning. More parameters and parameter definition available in `train_dreambooth_lora.py`.\n",
    "\n",
    "| Parameter | Definition |\n",
    "|-|-|  \n",
    "| base_model | Path to pretrained model or model identifier from huggingface.co/models. |\n",
    "| max_train_steps | Total number of training steps to perform.  If provided, overrides num_train_epochs. | \n",
    "| instance_prompt | The prompt with identifier specifying the instance |\n",
    "| validation_prompt | A prompt that is used during validation to verify that the model is learning.|\n",
    "| learning_rate | A prompt that is used during validation to verify that the model is learning. |\n",
    "| class_prompt | The prompt to specify images in the same class as provided instance images.|\n",
    "| class_data_dir |A folder containing the training data of class images. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"stabilityai/stable-diffusion-2-1\"\n",
    "n_steps = 1000\n",
    "instance_prompt = \"photo of <<TOK>>\"\n",
    "validation_prompt = \"photo of <<TOK>> sleeping on the coach\"\n",
    "learning_rate = 1e-4\n",
    "class_prompt = \"a photo of person\"\n",
    "class_data_dir = Path(\"/tmp/priors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `.ipynb_checkpoints` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the command to kick off the training. When training large AI models like Stable Diffusion, GPU memory becomes a key constraint. Here are some ways to help reduce memory usage during training:\n",
    "\n",
    "- Use gradient checkpointing: this method reduces memory by only storing a subset of activations during the forward pass, and recomputing them as needed during backpropagation. This trades off compute for memory. \n",
    "\n",
    "- Use quantization. 8-bit optimizers like those from BitsandBytes convert 32-bit floating point weights and activations to 8-bit during training. This reduces the memory usage but may hurt precision. To help wiht that BitsandBytes' optimizers automatically keep small, sensitive parameters at 32-bit. \n",
    "\n",
    "- Use xFormers a libnary for memory-effecient attention caluculation. Attention is often the memory bottleneck in large language models. It reduce the normal self-attention computation from O(n^2) to O(n), thus improve speed and reduce memory utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "command = f\"\"\"\n",
    "accelerate launch train_dreambooth_lora.py \\\n",
    "  --pretrained_model_name_or_path={base_model}  \\\n",
    "  --train_text_encoder \\\n",
    "  --instance_data_dir={dest_dir} \\\n",
    "  --class_data_dir={class_data_dir} \\\n",
    "  --output_dir={output_dir / \"lora\"} \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --instance_prompt=\"{instance_prompt}\" \\\n",
    "  --class_prompt=\"{class_prompt}\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --learning_rate={learning_rate} \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=100 \\\n",
    "  --max_train_steps={n_steps} \\\n",
    "  --num_class_images=200 \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision fp16 \\\n",
    "  --use_8bit_adam \\\n",
    "  --gradient_checkpointing \\\n",
    "  --validation_prompt=\"{validation_prompt}\" \\\n",
    "  --validation_epochs=50 \\\n",
    "  --seed=0 \\\n",
    "\"\"\"\n",
    "\n",
    "print(command)\n",
    "\n",
    "with open(output_dir / \"lora/train.sh\", \"w\") as f:\n",
    "    command_s = \" \".join(command.split())\n",
    "    f.write(command_s)\n",
    "\n",
    "res = subprocess.run(shlex.split(command))\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior-preservation in dreambooth\n",
    "\n",
    "Prior preservation is a technique used to avoid overfitting and language-drift. For prior preservation, you use other images of the same class as part of the training process. The nice thing is that you can generate those images using the Stable Diffusion model itself! The training script will save the generated images to a local path you specify.\n",
    "\n",
    "Let's checkout some of the class images generated by stable diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pior preservation images generated\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import random\n",
    "\n",
    "img_paths = [x for x in class_data_dir.iterdir() if x.is_file()]\n",
    "\n",
    "random_img = random.choice(img_paths)\n",
    "\n",
    "display(Image(filename=random_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Fine-tuned Model locally\n",
    "\n",
    "---\n",
    "Load the base Stable Diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import diffusers\n",
    "import torch \n",
    "from peft import PeftModel\n",
    "import os\n",
    "\n",
    "device=\"cuda\"\n",
    "\n",
    "pipe = diffusers.StableDiffusionPipeline.from_pretrained(base_model,\n",
    "                                                         cache_dir='hf_cache',\n",
    "                                                         torch_dtype=torch.float16,\n",
    "                                                         revision=\"fp16\")\n",
    "\n",
    "\n",
    "pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an image using the base SD model, then attach the LoRA adapter and generate a image use the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "prompt = \"\"\"\n",
    "photo of <<TOK>> front portrait, Pixar character, smiling, zoomed out, smooth skin, fun expression, tantalizing eyes, young and handsome, 4k\n",
    "\"\"\"\n",
    "\n",
    "negative_prompt = \"\"\"\n",
    "ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, \n",
    "watermark, grainy, signature, cut off, draft, amateur, multiple, gross, weird, uneven, furnishing, decorating, decoration, furniture, text, poor, low, basic, worst, juvenile, \n",
    "unprofessional, failure, crayon, oil, label, thousand hands\n",
    "\"\"\"\n",
    "\n",
    "seed = random.randint(1, 1000000000)\n",
    "generator = [torch.Generator(device=\"cuda\").manual_seed(seed)]\n",
    "\n",
    "print(seed)\n",
    "\n",
    "# Generate an image with base model\n",
    "             \n",
    "image = pipe(prompt, \n",
    "             num_inference_steps=50, \n",
    "             guidance_scale=7, \n",
    "             negative_prompt=negative_prompt,\n",
    "             generator=generator).images[0]\n",
    "\n",
    "images.append(image)\n",
    "\n",
    "\n",
    "# Attach LoRA weights\n",
    "\n",
    "pipe.load_lora_weights(output_dir / \"lora\", weight_name=\"pytorch_lora_weights.safetensors\")\n",
    "\n",
    "generator = [torch.Generator(device=\"cuda\").manual_seed(seed)]\n",
    "\n",
    "# Generate an image using fine tuned model with the same seed.\n",
    "\n",
    "image = pipe(prompt, \n",
    "             num_inference_steps=50, \n",
    "             guidance_scale=7, \n",
    "             negative_prompt=negative_prompt,\n",
    "             generator=generator).images[0]\n",
    "\n",
    "images.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the images side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot images side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "fig.suptitle(f\"Prompt\\n{prompt}\")\n",
    "ax1.imshow(images[0])\n",
    "ax1.set_title(\"Base Model\")\n",
    "ax2.imshow(images[1])\n",
    "ax2.set_title(\"Fine-tuned Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt Engineering** Recommend to start with just \"<< TOK >>\" or \"photo of << TOK >>\", this is the identifier used to fine tune the model. SD should identify your facial features with this identifier, and provide an image the resembles you. If not, you may need to provide additional image (better quality image). Or adjust the fine tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"<<TOK>>\"\n",
    "prompt = \"\"\"photo of <<TOK>> epic portrait, young and handsome, with glasses, zoomed out, blurred background cityscape, bokeh, perfect symmetry, by artgem, artstation ,concept art,cinematic lighting, highly detailed, \n",
    "octane, concept art, sharp focus, rockstar games,\n",
    "post processing, picture of the day, ambient lighting, epic composition\"\"\"\n",
    "# prompt = \"\"\"\n",
    "# photo of <<TOK>> front portrait, Pixar character, shocked, zoomed out, smooth skin, fun expression, tantalizing eyes, young and handsome, 4k\n",
    "# \"\"\"\n",
    "\n",
    "negative_prompt = \"\"\"\n",
    "ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, \n",
    "watermark, grainy, signature, cut off, draft, amateur, multiple, gross, weird, uneven, furnishing, decorating, decoration, furniture, text, poor, low, basic, worst, juvenile, \n",
    "unprofessional, failure, crayon, oil, label, thousand hands\n",
    "\"\"\"\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=7, negative_prompt=negative_prompt).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuse the LoRA Adapter and Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_model_dir = output_dir / \"fused_model\"\n",
    "\n",
    "fused_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "shutil.rmtree(fused_model_dir)\n",
    "\n",
    "pipe.fuse_lora()\n",
    "pipe.save_pretrained(fused_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the original pipeline and free up GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fused model back and make sure the performance is the same.\n",
    "\n",
    "We are doing the side by side comparison with base stable diffusion model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "\n",
    "pipe = diffusers.StableDiffusionPipeline.from_pretrained(base_model,\n",
    "                                                         cache_dir='hf_cache',\n",
    "                                                         torch_dtype=torch.float16,\n",
    "                                                         revision=\"fp16\")\n",
    "\n",
    "pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe2 = diffusers.StableDiffusionPipeline.from_pretrained(fused_model_dir,\n",
    "                                                         cache_dir='hf_cache',\n",
    "                                                         torch_dtype=torch.float16,\n",
    "                                                         revision=\"fp16\")\n",
    "\n",
    "pipe2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "\n",
    "prompt = \"\"\"\n",
    "photo of <<TOK>> pencil sketch, handsome, face front, centered\n",
    "\"\"\"\n",
    "\n",
    "negative_prompt = \"\"\"\n",
    "ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, \n",
    "watermark, grainy, signature, cut off, draft, amateur, multiple, gross, weird, uneven, furnishing, decorating, decoration, furniture, text, poor, low, basic, worst, juvenile, \n",
    "unprofessional, failure, crayon, oil, label, thousand hands\n",
    "\"\"\"\n",
    "\n",
    "seed = random.randint(1, 1000000000)\n",
    "generator = [torch.Generator(device=\"cuda\").manual_seed(seed)]\n",
    "\n",
    "print(seed)\n",
    "             \n",
    "image = pipe(prompt, \n",
    "             num_inference_steps=50, \n",
    "             guidance_scale=7, \n",
    "             negative_prompt=negative_prompt,\n",
    "             generator=generator).images[0]\n",
    "\n",
    "images.append(image)\n",
    "\n",
    "image = pipe2(prompt, \n",
    "             num_inference_steps=50, \n",
    "             guidance_scale=7, \n",
    "             negative_prompt=negative_prompt,\n",
    "             generator=generator).images[0]\n",
    "\n",
    "images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot images side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "fig.suptitle(f\"Prompt\\n{prompt}\")\n",
    "ax1.imshow(images[0])\n",
    "ax1.set_title(\"Base Model\")\n",
    "ax2.imshow(images[1])\n",
    "ax2.set_title(\"Fine-tuned Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe, pipe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
