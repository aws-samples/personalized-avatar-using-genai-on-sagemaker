{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6482139c",
   "metadata": {},
   "source": [
    "# Deploy Stable Diffusion using Triton Inference Server\n",
    "\n",
    "In this notebook we will host LoRA finetuned Stable Diffusion models on Triton Inference Server provided by NVIDIA\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Warning</b>: You should run this notebook on a SageMaker Notebook Instance. An GPU instance such as `ml.g5.2xlarge` is recommended. This notebook is tested on `conda_python_p310` kernel. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bb967",
   "metadata": {},
   "source": [
    "### Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e6d37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]\n",
    "!pip install -U sagemaker pywidgets numpy PIL\n",
    "!pip install -Uq conda-pack==0.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import *\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "# variables\n",
    "s3_client = boto3.client(\"s3\")\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee241ad2",
   "metadata": {},
   "source": [
    "## What is Triton Inference Server\n",
    "\n",
    "**Triton Inference Server** is an open source inference serving toolkit from NVIDIA that supports high-performance inferencing for deep learning models. It provides a framework-agnostic platform to deploy trained AI models from any framework, including TensorFlow, PyTorch, and ONNX. Triton allows multiple models to be served from the same server, optimizing hardware utilization.\n",
    "\n",
    "**The Triton backend for Python.** The goal of Python backend is to let you serve models written in Python by Triton Inference Server without having to write any C++ code. Read [here](https://github.com/triton-inference-server/python_backend) for more information\n",
    "\n",
    "In this example, two fine tuned stable diffusion models are already prepared for you. Take a look at the `model_repository` folder structure.\n",
    "\n",
    "```\n",
    "model_repository\n",
    "└── james                                       # model folder\n",
    "    ├── 1                                       # model version\n",
    "    │   └── model.py                            # inference handler functions must save in this python file\n",
    "    │   └── pytorch_lora_weights.safetensors    # LoRA adapter weights\n",
    "    ├── config.pbtxt                            # model configuration\n",
    "    └── sd_env.tar.gz                           # custom execution environment (Created in next section)\n",
    "└── diwakar\n",
    "    ├── 1\n",
    "    │   └── model.py\n",
    "    │   └── pytorch_lora_weights.safetensors\n",
    "    ├── config.pbtxt\n",
    "    └── sd_env.tar.gz \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c0047e",
   "metadata": {},
   "source": [
    "Packaging a conda environment\n",
    "\n",
    "When using the Triton Python backend, you can include your own environment and dependencies. The recommended way to do this is to use [conda pack](https://conda.github.io/conda-pack/) to generate a conda environment archive in `tar.gz` format, include it in your model repository, and point to it in the `config.pbtxt` file of python models that should use it, adding the snippet: \n",
    "\n",
    "```\n",
    "parameters: {\n",
    "  key: \"EXECUTION_ENV_PATH\",\n",
    "  value: {string_value: \"$$TRITON_MODEL_DIRECTORY/your_env.tar.gz\"}\n",
    "}\n",
    "\n",
    "```\n",
    "Let's create this file and save it to the pipeline model repo, which is our business logic \"model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e47a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile environment.yml\n",
    "name: sd_env\n",
    "dependencies:\n",
    "  - python=3.10\n",
    "  - pip\n",
    "  - pip:\n",
    "      - numpy\n",
    "      - --extra-index-url https://download.pytorch.org/whl/cu118 torch\n",
    "      - accelerate==0.22.0\n",
    "      - transformers==4.26\n",
    "      - diffusers==0.21.4\n",
    "      - xformers\n",
    "      - bitsandbytes\n",
    "      - conda-pack==0.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9288f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc47f16",
   "metadata": {},
   "source": [
    "We will use the same conda environment for both models. In reality they can be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda pack -n sd_env -o model_repository/james/sd_env.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ca52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp model_repository/james/sd_env.tar.gz model_repository/diwakar/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d34740",
   "metadata": {},
   "source": [
    "Let's checkout the `model.py` inference script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8028fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize model_repository/james/1/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebbb03",
   "metadata": {},
   "source": [
    "## Test of Triton model repository\n",
    "you can test the model repository and validate it is working. Let's run the Triton docker container locally and invoke the script to check this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1693de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df256fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"model_repository\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9ef7d",
   "metadata": {},
   "source": [
    "We are running the Triton container in detached model with the `-d` flag so that it runs in the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53edeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --gpus=all -d --shm-size=4G --rm -p8000:8000 -p8001:8001 -p8002:8002 -v$(pwd)/$repo_name:/model_repository nvcr.io/nvidia/tritonserver:23.10-py3 tritonserver --model-repository=/model_repository --exit-on-error=false\n",
    "time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ccc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_ID=!docker container ls -q\n",
    "FIRST_CONTAINER_ID = CONTAINER_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $FIRST_CONTAINER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baaa6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker logs $FIRST_CONTAINER_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a9e7c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Warning</b>: Rerun the cell above to check the container logs until you verify that Triton has loaded all models successfully, otherwise inference request will fail.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da4c0aa",
   "metadata": {},
   "source": [
    "#### Now we will invoke the script locally\n",
    "\n",
    "We will use Triton's HTTP client and its utility functions to send a request to `localhost:8000`, where the server is listening. We are sending text as binary data for input and receiving an array that we decode with numpy as output. Check out the code in `model_repository/pipeline/1/model.py` to understand how the input data is decoded and the output data returned, and check out more Triton Python backend [docs](https://github.com/triton-inference-server/python_backend) and [examples](https://github.com/triton-inference-server/python_backend/tree/main/examples) to understand how to handle other data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b3a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = httpclient.InferenceServerClient(url=\"localhost:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89194083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "prompt = \"\"\"photo of <<TOK>> epic portrait, handsome, zoomed out, blurred background cityscape, bokeh, perfect symmetry, by artgem, artstation ,concept art,cinematic lighting, highly detailed, \n",
    "octane, concept art, sharp focus, rockstar games,\n",
    "post processing, picture of the day, ambient lighting, epic composition\"\"\"\n",
    "\n",
    "negative_prompt = \"\"\"\n",
    "beard, goatee, ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, \n",
    "watermark, grainy, signature, cut off, draft, amateur, multiple, gross, weird, uneven, furnishing, decorating, decoration, furniture, text, poor, low, basic, worst, juvenile, \n",
    "unprofessional, failure, crayon, oil, label, thousand hands\n",
    "\"\"\"\n",
    "\n",
    "seed = 233571759 #random.randint(1, 1000000000)\n",
    "gen_args = json.dumps(dict(num_inference_steps=50, guidance_scale=7, seed=seed))\n",
    "\n",
    "input_dict = dict(prompt = prompt,\n",
    "              negative_prompt = negative_prompt,\n",
    "              gen_args = gen_args)\n",
    "inputs = []\n",
    "for name, data in input_dict.items():\n",
    "    \n",
    "    obj = np.array([data], dtype=\"object\").reshape((-1, 1))\n",
    "\n",
    "    i = httpclient.InferInput(name, obj.shape, np_to_triton_dtype(obj.dtype))\n",
    "    i.set_data_from_numpy(obj)\n",
    "    inputs.append(i)\n",
    "\n",
    "output_img = httpclient.InferRequestedOutput(\"generated_image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73e64c",
   "metadata": {},
   "source": [
    "Change your target model. Available models: [james, diwakar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b809f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = \"diwakar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "query_response = client.infer(model_name=target_model, inputs=inputs, outputs=[output_img])\n",
    "\n",
    "print(f\"took {time.time()-start} seconds\")\n",
    "\n",
    "image = query_response.as_numpy(\"generated_image\")\n",
    "\n",
    "test = np.squeeze(image).tolist()\n",
    "Image.open(BytesIO(base64.b64decode(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaad57f",
   "metadata": {},
   "source": [
    "Check your memory utilization using `nvidia-smi`, a command line utility that helps with managing NVIDIA Graphics Processing Unit (GPU) devices.\n",
    "\n",
    "You can fit up-to 4 stable diffusion 2.1 models on a single A10G GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcea5f5",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker kill $FIRST_CONTAINER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2b1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
